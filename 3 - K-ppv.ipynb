{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Un peu de théorie sur k-PPV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motivation intuitive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'algorithme des $k$ plus proches voisins ($k$-PPV) est certainement un des algorithmes les plus simples d'apprentissage automatique. Il est motivé par l'idée que des *entrées* $x_t$ semblables devraient avoir des *cibles* $y_t$ semblables. Ainsi, pour bien définir un algorithme $k$-PPV, il suffit de définir ce que veut dire *semblable* dans le contexte des entrées et de définir l'influence de ces voisins sur la prédiction de la cible pour une entrée de test.\n",
    "\n",
    "Donc, pour obtenir une prédiction de la cible pour une entrée de test $x$, il suffit de trouver les k plus proches voisins selon une métrique déterminant jusqu'à quel point des entrées sont semblables (par exemple, la distance euclédienne ou norme $L^2$, ou de façon plus générale la norme $L^p$ de Minkowski) et d'utiliser ces $k$ plus proches voisins pour prédire la cible de $x$. Dans un problème de classification, la prédiction correspond à la classe majoritaire parmi les $k$ plus proches voisins, i.e. que l'ensemble des $k$ plus proches voisins votent pour la classe correspondant à leur cible respective et la classe recueillant le plus de vote est choisie en tant que prédiction par l'algorithme."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Formalisation mathématique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Soit\n",
    "\n",
    "* $x$ une entrée de test\n",
    "* $m$ le nombre de classes\n",
    "* $D_n = \\{(x_t,y_t)\\}_{t=1}^n$ l'ensemble d'entraînement, où $y \\in Y=\\{1,\\dots,m\\}$ correspond à l'identité de la classe cible de l'entrée $x_t$\n",
    "* $d(\\dot{},\\dot{})$ une fonction de distance\n",
    "* $V(x,T,d(\\dot{},\\dot{}),k)$ l'ensemble des $k$ plus proches voisins de $x$ parmi les entrées de $T$ ainsi que leur cible associée"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La prédiction de classification par l'algorithme des k plus proches voisins est donc:\n",
    "\n",
    "> $$f(x)={\\mbox{arg max}} \\left(\\frac{1}{k} \\sum_{(x_i,y_i) \\in V(x)} \\mathrm{onehot}_{m}(y_i)\\right)$$\n",
    "\n",
    "Une fonction de distance couramment utilisée est la distance euclédienne:\n",
    "\n",
    "> $$d(a,b)= \\sqrt{\\sum_{i=1}^d(a_i-b_i)^2}$$\n",
    "\n",
    "qui est un cas spécifique, avec $p=2$, de la norme $L^p$ de Minkowski:\n",
    "\n",
    "> $$d(a,b)= \\left(\\sum_{i=1}^d|a_i-b_i|^p\\right)^\\frac{1}{p}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pseudo-code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On définit un algorithme d'apprentissage en précisant sa procédure d'entraînement et de prédiction pour une nouvelle entrée de test. Étant donné que la procédure d'entraînement de l'algorithme $k$-PPV consiste simplement à mettre en mémoire l'ensemble d'entraînement $D_n$, voici donc la procédure de prédiction dans le cas particulier ou k=1:\n",
    "\n",
    "    definition 1-PPV(x)\n",
    "        min = +inf # Initialisation de la distance du plus proche voisin\n",
    "        ppv = -1 # Initialisation de l'indice du plus proche voisin\n",
    "        \n",
    "        pour t=1 à n faire\n",
    "            dt = d(X[t], x)\n",
    "            si dt < min alors\n",
    "                min = dt\n",
    "                ppv = t\n",
    "                \n",
    "        retourner Y[ppv]\n",
    "        \n",
    "La prédiction s'exécute en temps $O(n(k+d))$. Il est cependant possible d'obtenir un temps d'exécution dans $O(n(log(k)+d))$, en utilisant une queue de priorité (monceau)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mise en pratique!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## En guise d'introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On vous demande de concevoir un algorithme d'apprentissage permettant d'identifier des fleurs sur un convoyeur. Il s'agit de trois variétés d'iris. Le convoyeur est doté d'une caméra capable de mesurer les longueurs et largeurs des pétales et sépales de chaque fleur. C'est à partir de ces caractéristiques (traits) que vous devez déterminer la sorte de chaque fleur (la classe). Vous ne connaissez rien aux fleurs! Fort heureusement vous disposez d'un ensemble d'entraînement associant à divers exemples de mesures d'iris la bonne variété (classe). Armé de l'algo 1-PPV et de Python vous foncez tête baissée."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comment calculer une distance $L^p$ (Minkowski) entre deux vecteurs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Passons maintenant aux choses sérieuses. On désire obtenir la fonction `minkowski_vec` qui nous permet de comparer deux fleurs sur la base de leurs traits. Complétez la fonction suivante, puis testez-la sur deux exemples d'iris (revoir au besoin la démo 2 pour l'accès et l'importation de l'ensemble de données iris). \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def minkowski_vec(x1,x2,p=2.0):\n",
    "    dist = 1\n",
    "    return dist # cette ligne permet de retourner le résultat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rappelez-vous la [définition](http://en.wikipedia.org/wiki/Minkowski_distance) de la distance. \n",
    "\n",
    "**Chose importante:** on peut calculer cette distance en itérant sur chaque composante de x1 et x2, et en calculant la somme après, ou on peut profiter du fait que la plupart (ou même toutes) des opérations mathématiques (abs, +, -, \\*\\* etc.) sur des structures de données itérables (listes, vecteurs/matrices) produisent des résultats équivalents à l'application d'une boucle for, mais en beaucoup moins de temps (on parle des langages interprétés comme Python). Par exemple, on peut calculer la somme de la différence des valeurs absolues de x1 et x2 comme\n",
    "\n",
    "    s = 0\n",
    "    for i in range(x1.shape[0]):\n",
    "        s = s + abs(x1[i] - x2[i])\n",
    "\n",
    "ou simplement\n",
    "\n",
    "    s = numpy.sum(numpy.abs(x1 - x2))\n",
    "\n",
    "En plus d'être plus compacte, la deuxième option est beaucoup plus rapide (parce qu'elle fait appel à une implémentation efficace de sum et abs en C++).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comment calculer une distance $L^p$ entre un vecteur et une matrice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il nous faut aussi une fonction qui va nous permettre de comparer une fleur avec tout un ensemble de fleurs, sur la base de leurs traits. On va maintenant modifier la fonction `minkowski` pour calculer une *distance* $L^p$ entre un vecteur et une matrice (c.a.d. une fonction qui va nous retourner un vecteur de distances $L^p$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def minkowski_mat(x,Y,p=2.0):\n",
    "    dist = # completez \n",
    "    return dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comme pour `minkowski_vec`, il y a deux manières de procéder:\n",
    " \n",
    "1. *Simple et inefficace:* en écrivant une boucle qui appelle la fonction `minkowski_vec(x,Y[i,:],p)` et garde les résultats dans le vecteur `dist`.\n",
    "2. *Plus compliqué mais plus efficace:* en utilisant le fait que `numpy` fait quelque chose d'intelligent quand il évalue `x-Y`, car il retourne une matrice qui contient le vecteur `x-Y[i,:]` sur la rangée $i$ (le mécanisme qui rend cela possible s'appelle *broadcasting*). Voici une solution:\n",
    "        def minkowski_mat(x,Y,p=2.0):\n",
    "            diff = x - Y # diff sera une matrice\n",
    "            absdiff = abs(diff) # absdiff sera une matrice\n",
    "            powdiff = absdiff**p # powdiff sera une matrice\n",
    "            s = numpy.sum(powdiff,axis=1) # calcule la somme de chaque rangée, s est un vecteur\n",
    "            dist = s**(1.0/p) # dist sera un vecteur aussi\n",
    "            return dist\n",
    "\n",
    "    ou bien\n",
    "\n",
    "        def minkowski_mat(x,Y,p=2.0):\n",
    "            return (numpy.sum((abs(x-Y))**p,axis=1))**(1.0/p) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encore une fois, chose importante à retenir:  \n",
    "**la  grande majorité des opérations vecteur-vecteur, vecteur-matrice ou bien matrice-matrice seront beaucoup plus efficaces en utilisant les\n",
    "opérateurs numpy au lieu d'une boucle for.** (les raisons étant,  entre autres, le fait que python est un langage interprété et que numpy a  des implémentations très efficace pour certaines opérations vectorielles).\n",
    " \n",
    "Vous avez peut-être remarqué que la différence entre les implémentations efficaces de `minkowski_vec` et `minkowski_mat` est seulement la partie: `axis=1`. L'exercice est de comprendre pourquoi il est nécessaire de spécifier sur quel *axe* on va faire la somme."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-PPV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enfin, nous y sommes! Complétez la fonction suivante qui predit l'espece de l'iris decrite par les caracteristiques x et vérifiez son efficacité. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def ppv(x,data,p=2): \n",
    "    # accéder et conserver dans une variable (caract) les caractéristiques (sans étiquette) de data.\n",
    "    dist = minkowski_mat(x, caract, p)\n",
    "    # trouver l'indice de la distance minimum \n",
    "    # retourner la classe (dernière colonne iris) correspondant à l'indice trouvé"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "À noter ici que `x` est le vecteur de caractéristiques (sans étiquette) de l'exemple de test. Ayant en mains la fonction `minkowski_mat`, les choses devrait être simple, car dist va contenir un vecteur/liste des distances. En utilisant `numpy.argmin` on va trouver l'exemple/la fleur qui est la plus *proche* (dans le sens de minkowski) de `x` et ainsi, on va conclure que l'étiquette (prédite) de `x` est celle de cet exemple."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous avons maintenant en notre possession toutes les composantes de l'algorithme 1-PPV. Il reste simplement à en faire l'assemblage et à tester le tout. \n",
    "\n",
    ">Rappelez vous que les fonctions définies dans les cellules de code précédentes sont accessibles dans toutes les cellules subséquentes une fois les précédentes exécutés.\n",
    "\n",
    "Afin de tester votre implémentation, écrivez une boucle `for` qui appelle, pour chaque exemple `i`, la fonction `ppv(iris[i,:-1],iris,p)` et qui compare la classe prédite avec `iris[i,-1]` (la vrai étiquette). Les deux devraient toujours être les mêmes (pourquoi?)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "iris = np.loadtxt('iris.txt')\n",
    "\n",
    "# complétez"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus / trucs auxquels réfléchir pour la prochaine fois"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Divisez l'ensemble iris en deux - un ensemble d'entraînement qui contient 100 exemples (un sous-ensemble aléatoire!) et un ensemble de test qui contient le reste.\n",
    "  * Utilisez le premier sous-ensemble comme données sur lesquelles on va calculer les distances minkowski (donc données d'entraînement).\n",
    "  * Calculez la performance de votre algorithme sur les deux ensembles. Pourquoi y a-t-il une telle différence?               \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Implémentez l'algorithme $k$-PPV avec $k > 1$\n",
    "   * Trouvez le $k$ qui donne la meilleure performance sur les deux ensembles - expliquez la différence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Que se passe-t-il lorsque $k=100$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
