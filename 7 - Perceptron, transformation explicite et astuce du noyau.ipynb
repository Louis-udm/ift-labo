{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Premièrement, amusons nous un peu avec des maths. \n",
    "\n",
    "Pour appliquer la descente de gradient, on a évidemment besoin de calculer le gradient de notre fonction de coût (ou perte). Supposons le perceptron suivant avec un rectifieur comme couche d'activation.\n",
    "\n",
    "$y = w^Tx+b$\n",
    "\n",
    "$a = rect(y)$ où $rect(x) = \\left\\{ \n",
    "  \\begin{array}{l l}\n",
    "    0 & \\quad \\text{si $x\\le0$}\\\\\n",
    "    x & \\quad \\text{sinon}\n",
    "  \\end{array} \\right.$\n",
    "  \n",
    "Normalement on utiliserait un softmax pour avoir un vecteur de probabilité, mais j'utilise ici un exemple bidon pour vous faire pratiquer quelques principes de dérivation.\n",
    "\n",
    "Supposons maintenant qu'on utilise comme fonction de coût la différence au carrée, soit:\n",
    "\n",
    "$L(a,t) = (a-t)^2$\n",
    "\n",
    "Si on remplace les variables on obtient \n",
    "\n",
    "$\\displaystyle L(a,t) = (rect(w^Tx+b)-t)^2$\n",
    "\n",
    "Ce qui est encore supportable pour les yeux, mais comme vous avez pu le voir dans le devoir, ça peut vite dégénérer. L'idéal, lorsqu'on dérive notre fonction de coût, c'est de tirer profit de ce qu'on appelle la dérivation en chaîne. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rappel sur les dérivations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Petit rappel: La règle de dérivation en chaîne nous dit que \n",
    "\n",
    "$\\displaystyle \\frac{\\partial y}{\\partial x} = \\frac{\\partial y}{\\partial u}\\frac{\\partial u}{\\partial x}$ ($y$, $x$ et $u$ n'ont rien à voir avec nos formules.)\n",
    "\n",
    "Alors si on applique la règle à notre problème, on obtient :\n",
    "\n",
    "$\\displaystyle \\frac{\\partial L(a,t)}{\\partial w} = \\frac{\\partial L(a,t)}{\\partial a}\\frac{\\partial a}{\\partial y}\\frac{\\partial y}{\\partial w}$\n",
    "\n",
    "Autrement dit, pas besoin de se battre (et de possiblement faire 1001 erreurs de calculs) avec la formule complète de $\\frac{\\partial L(a,t)}{\\partial w}$, on peut calculer séparément $\\frac{\\partial L(a,t)}{\\partial a}$, $\\frac{\\partial a}{\\partial y}$ et $\\frac{\\partial y}{\\partial w}$. \n",
    "\n",
    "Rappelez vous de cette règle, elle vous sera nécessaire plus tard (pas aujourd'hui) pour la back-propagation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dérivation de w^Tx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lorsqu'on dérive une opération vectorielle ou matricielle tel que $w^Tx$, il est toujours mieux de développer l'équation. Dans notre cas,\n",
    "\n",
    "$\\displaystyle w^Tx = \\sum_{i=1}^{d} w_ix_i$\n",
    "\n",
    "Ainsi, la dérivé n'est pas très compliqué.\n",
    "\n",
    "$\\displaystyle \\frac{\\partial}{\\partial w}w^Tx = \\frac{\\partial}{\\partial w}\\sum_{i=1}^{d} w_ix_i = \\sum_{i=1}^{d} \\frac{\\partial}{\\partial w} w_ix_i$\n",
    "\n",
    "Mais attention! $\\frac{\\partial}{\\partial w}w_ix_i \\neq x_i$, car on a bien $\\frac{\\partial}{\\partial w}$ et non pas $\\frac{\\partial}{\\partial w_i}$. Il faut donc y aller au cas par cas et calculer pour chaque dimension. \n",
    "\n",
    "$\\displaystyle \\left(\\frac{\\partial}{\\partial w_1}w^Tx,\\frac{\\partial}{\\partial w_2}w^Tx,\\dots,\\frac{\\partial}{\\partial w_n}w^Tx\\right) = \\left(\\sum_{i=1}^d \\frac{\\partial}{\\partial w_1} w_ix_i,\\sum_{i=1}^d \\frac{\\partial}{\\partial w_2} w_ix_i,\\dots,\\sum_{i=1}^d \\frac{\\partial}{\\partial w_n} w_ix_i\\right)$\n",
    "\n",
    "Autrement dit \n",
    "\n",
    "$\\displaystyle \\frac{\\partial}{\\partial w_i}w^Tx = \\sum_{j=1}^d \\frac{\\partial}{\\partial w_i} w_jx_j$\n",
    "\n",
    "Bon, j'en ai trop dit, à vous de terminer la dérivation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dérivation particulière"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il arrive parfois qu'on se retrouve face à une fonction qui semble impossible à dériver, comme par exemple la fonction indicatrice ou $I_{qqc}$ ou bien la fonction $rect(x)$. Une manière simple de règler le problème est de «casser» l'équation en deux. \n",
    "\n",
    "Exemple: $rect(y)$\n",
    "\n",
    "On sait que $rect(x) = \\left\\{ \n",
    "  \\begin{array}{l l}\n",
    "    0 & \\quad \\text{si $x\\le0$}\\\\\n",
    "    x & \\quad \\text{sinon}\n",
    "  \\end{array} \\right.$\n",
    "  \n",
    "Si on remplace $x$ par l'équation $wx+b$, on obtient\n",
    "\n",
    "$rect(wx+b) = \\left\\{ \n",
    "  \\begin{array}{l l}\n",
    "    0 & \\quad \\text{si $w^Tx+b\\le0$}\\\\\n",
    "    wx+b & \\quad \\text{sinon}\n",
    "  \\end{array} \\right.$\n",
    "\n",
    "Il ne reste plus qu'à remonter d'un niveau et on obtient \n",
    "\n",
    "$\\displaystyle L(y,t) = \\left\\{ \n",
    "  \\begin{array}{l l}\n",
    "    t^2 & \\quad \\text{si $w^Tx+b\\le0$}\\\\\n",
    "    (w^Tx+b-t)^2 & \\quad \\text{sinon}\n",
    "  \\end{array} \\right.$\n",
    "\n",
    "On peut maintenant dériver $t^2$ et $(w^Tx+b-t)^2$. N'oubliez pas la dérivation en chaîne. La deuxième équation peut être écrite $(y-t)^2$.\n",
    "\n",
    "Maintenant, à vos crayons, prêts, dérivez $\\displaystyle\\frac{\\partial L(a,t)}{\\partial w}$!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformations non-linéaires"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour démontrer l'utilité des transformations non-linéaires, nous allons utiliser les ensembles de données cercle et ellipse, disponible sur la page du site. On va premièrement appliquer une transformation sur les données avant l'entraînement. L'algorithme ne touchera que les données transformées. On ne se cassera pas la tête pour commencer, implémentez simplement une transformation polynomiale de deuxième degré.\n",
    "\n",
    "Petit rappel:\n",
    "\n",
    "$\\phi_{\\text{poly}^2}(x) = \\left(x_1,x_2,\\dots,x_d,\n",
    "\\alpha_{11} x_1^2, \\alpha_{22}x_2^2, \\dots, \\alpha_{dd}x_d^2,\n",
    "\\alpha_{12}x_1x_2, \\alpha_{13}x_1x_2, \\dots, \\alpha_{1d}x_1x_d, \\dots, \\alpha_{(d-1)d}x_{d-1}x_d\\right)$\n",
    "\n",
    "Pour faire simple, on peut mettre $\\alpha_i=1$ $\\forall i$\n",
    "\n",
    "Et pour faire encore plus simple, les données cercles et ellipse sont seulement en 2-d..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "import numpy as np\n",
    "import utilitaires\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# prends une matrice de données en entrée (sans cibles) et renvoie une matrice transformée avec une polynomiale de degrée p\n",
    "def polynome(x,p=2):\n",
    "    new_elements = # À vous d'implémenter les nouveaux éléments (ça peut très bien pres plus d'une ligne de code)\n",
    "    new_x = np.concatenate((x,new_elements),axis=1)\n",
    "    return new_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class perceptron:\n",
    "\tdef __init__(self, mu):\n",
    "\t\tself.mu = mu\n",
    "    \n",
    "\tdef train(self, train_data):\n",
    "          nb_example = train_data.shape[0]\n",
    "\n",
    "          self.weights = np.random.random(train_data.shape[1])\n",
    "          self.weights[-1] = 0\n",
    "          datas = np.array(train_data)\n",
    "          datas[:,-1] = 1\n",
    "          i = 0\n",
    "          count = 0 # We stop when the set is linearly separated\n",
    "          n_iter = 0\n",
    "          n_iter_max = nb_example*100\n",
    "          while (count < nb_example and n_iter < n_iter_max):\n",
    "            if (np.dot(datas[i], self.weights)) * train_data[i,-1] < 0:\n",
    "              self.weights += self.mu * train_data[i,-1] * datas[i]\n",
    "              count = 0\n",
    "            else:\n",
    "              count = count + 1\n",
    "            i = (i + 1) % nb_example\n",
    "            n_iter += 1\n",
    "\n",
    "\tdef compute_predictions(self, test_data):\n",
    "\n",
    "           sorties = []\n",
    "           for i in range(len(test_data)):\n",
    "             data = []\n",
    "             for j in range(len(test_data[i])):\n",
    "               data.append(test_data[i][j])\n",
    "             data.append(1)\n",
    "             sorties.append(np.dot(data, self.weights))\n",
    "           return sorties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# On commence par charger les données\n",
    "data = np.loadtxt('cercle.txt')\n",
    "#data = np.loadtxt('ellipse.txt')\n",
    "\n",
    "# Il n'y a que deux dimensions...\n",
    "train_cols = [0,1]\n",
    "# Une variable pour contenir l'indice de la colonne correspondant aux étiquettes.\n",
    "target_ind = [data.shape[1] - 1]\n",
    "\n",
    "# Nombre de classes\n",
    "n_classes = 2\n",
    "# Nombre de points d'entrainement\n",
    "n_train = 1500\n",
    "# Taille de la grille = grid_size x grid_size\n",
    "grid_size = 50\n",
    "\n",
    "print \"On va entrainer un perceptron sur \", n_train, \" exemples d'entrainement\"\n",
    "\n",
    "# decommenter pour avoir des resultats non-deterministes \n",
    "random.seed(3395)\n",
    "\n",
    "# Déterminer au hasard des indices pour les exemples d'entrainement et de test\n",
    "inds = range(data.shape[0])\n",
    "random.shuffle(inds)\n",
    "train_inds = inds[:n_train]\n",
    "test_inds = inds[n_train:]\n",
    "    \n",
    "# Separer les donnees dans les deux ensembles: entrainement et test.\n",
    "train_set = data[train_inds,:]\t# garder les bonnes lignes\n",
    "train_set = train_set[:,train_cols + target_ind]  # garder les bonnes colonnes\n",
    "test_set = data[test_inds,:]\n",
    "test_set = test_set[:,train_cols + target_ind]\n",
    "\n",
    "# Separarer l'ensemble de test: entrees et etiquettes.\n",
    "test_inputs = test_set[:,:-1]\n",
    "test_labels = test_set[:,-1]\n",
    "\n",
    "# Le taux d'apprentissage\n",
    "mu = 0.005\n",
    "\n",
    "# Transforme les données\n",
    "transformed_train_set = np.concatenate((polynome(train_set[:,:-1]),train_set[:,-1][:,None]),axis=1)\n",
    "transformed_test_inputs = polynome(test_inputs)\n",
    "\n",
    "# Créer et entrainer le modele\n",
    "model_perceptron = perceptron(mu)\n",
    "model_perceptron.train(transformed_train_set)\n",
    "\n",
    "# Obtenir les sorties sur l'ensemble de test.\n",
    "t1 = time.clock()\n",
    "les_sorties = model_perceptron.compute_predictions(transformed_test_inputs)\n",
    "t2 = time.clock()\n",
    "print 'Ca nous a pris ', t2-t1, ' secondes pour calculer les predictions sur ', transformed_test_inputs.shape[0],' points de test'\n",
    "\n",
    "# Convertir les sorties en classe. On prend le signe.\n",
    "classes_pred = np.sign(les_sorties)\n",
    "   \n",
    "# Mesurer la performance.\n",
    "err = 1.0 - np.mean(test_labels==classes_pred)\n",
    "print \"L'erreur de test est de \", 100.0 * err,\"%\"\n",
    "\n",
    "# Affichage graphique\n",
    "if len(train_cols) == 2:\n",
    "    # Surface de decision\n",
    "    t1 = time.clock()\n",
    "    les_sorties = model_perceptron.compute_predictions(transformed_train_set[:,:-1])\n",
    "    # Convertir les sorties en classe. On prend le signe.\n",
    "    train_classes_pred = np.sign(les_sorties)\n",
    "    plt.scatter(train_set[:,0],train_set[:,1],c=train_classes_pred)\n",
    "    plt.scatter(test_set[:,0],test_set[:,1],c=classes_pred)\n",
    "    plt.show()\n",
    "    \n",
    "    plt.scatter(train_set[:,0],train_set[:,1],c=train_set[:,-1])\n",
    "    plt.scatter(test_set[:,0],test_set[:,1],c=test_labels)\n",
    "    plt.show()\n",
    "\n",
    "    t2 = time.clock()\n",
    "    print 'Ca nous a pris ', t2-t1, ' secondes pour calculer les predictions sur ', grid_size * grid_size, ' points de la grille'\n",
    "    filename = 'grille_' + '_c1=' + str(train_cols[0]) + '_c2=' + str(train_cols[1])+'.png'\n",
    "    print 'On va sauvegarder la figure dans ', filename\n",
    "    pylab.savefig(filename,format='png')\n",
    "        \n",
    "else:\n",
    "    print 'Trop de dimensions (', len(train_cols),') pour pouvoir afficher la surface de decision'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Astuce du noyau"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Votre objectif pour cette section sera d'implanter un perceptron à noyau gaussien. On vous demande d'utiliser un kernel gaussien pour ce problème. Pour revoir les bases, référez-vous au [document du cours](https://studium.umontreal.ca/mod/resource/view.php?id=416954), notamment la deuxième partie. Vous pouvez vous aider avec le pseudo-code [ici](http://cvit.iiit.ac.in/thesis/ranjeethMS2007/thesis/node21.html).\n",
    "\n",
    "Commencez par implémenter une fonction de kernel gaussien qui pourra vous servir pour faire le calcul initial. Il ne devrait pas être nécessaire de rappeler la fonction à chaque époque de l'entraînement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def kernel(?):\n",
    "    k = np.zeros((?,?))\n",
    "    return k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vous pouvez ensuite compléter le code suivant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class KernelPerceptron:\n",
    "    def __init__(self, mu):\n",
    "        self.mu = mu\n",
    "\n",
    "    def train(self, train_data, kernel_function):\n",
    "        nb_example = train_data.shape[0]\n",
    "        \n",
    "        # Initialise les alphas\n",
    "        \n",
    "        # Retire les cibles\n",
    "        datas = np.array(train_data)[:,:-1]\n",
    "        \n",
    "        # Calcul du kernel\n",
    "    \n",
    "        i = 0\n",
    "        count = 0 \n",
    "        n_iter = 0\n",
    "        n_iter_max = nb_example*100\n",
    "        while (count < nb_example and n_iter < n_iter_max):\n",
    "            # comment calculer la prédiction?\n",
    "            if ?*train_data[i,-1] < 0:\n",
    "                self.alphas += # comment alpha est mis à jour?\n",
    "                count = 0\n",
    "            else:\n",
    "                count = count + 1\n",
    "            i = (i + 1) % nb_example\n",
    "            n_iter += 1\n",
    "\n",
    "    def compute_predictions(self, test_data, kernel_function):\n",
    "        # Il faut calculer le kernel\n",
    "        \n",
    "        sorties = []\n",
    "        for i in range(len(test_data)):\n",
    "            data = []\n",
    "            for j in range(len(test_data[i])):\n",
    "               data.append(test_data[i][j])\n",
    "            data.append(1)\n",
    "            \n",
    "            # comment calculer la prédiction?\n",
    "            sorties.append(?)\n",
    "        return sorties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# charger les donnees\n",
    "#data = np.loadtxt('ellipse.txt')\n",
    "data = np.loadtxt('cercle.txt')\n",
    "type_transformation=2\n",
    "\n",
    "# Les colonnes (traits/caracteristiques) sur lesqueles on va entrainer notre modele\n",
    "# Pour que gridplot fonctionne, len(train_cols) devrait etre 2\n",
    "train_cols = [0,1]\n",
    "# L'indice de la colonne contenant les etiquettes\n",
    "target_ind = [data.shape[1] - 1]\n",
    "\n",
    "# Nombre de classes\n",
    "n_classes = 2\n",
    "# Nombre de points d'entrainement\n",
    "n_train = 1500\n",
    "# Taille de la grille = grid_size x grid_size\n",
    "grid_size = 50\n",
    "\n",
    "print \"On va entrainer un algo lineaire sur \", n_train, \" exemples d'entrainement\"\n",
    "\n",
    "# decommenter pour avoir des resultats non-deterministes \n",
    "random.seed(3395)\n",
    "# DÃ©terminer au hasard des indices pour les exemples d'entrainement et de test\n",
    "inds = range(data.shape[0])\n",
    "random.shuffle(inds)\n",
    "train_inds = inds[:n_train]\n",
    "test_inds = inds[n_train:]\n",
    "\n",
    "# separer les donnees dans les deux ensembles\n",
    "train_set = data[train_inds,:]\n",
    "train_set = train_set[:,train_cols + target_ind]\n",
    "test_set = data[test_inds,:]\n",
    "test_set = test_set[:,train_cols + target_ind]\n",
    "\n",
    "# separarer l'ensemble de test dans les entrees et les etiquettes\n",
    "test_inputs = test_set[:,:-1]\n",
    "test_labels = test_set[:,-1]\n",
    "\n",
    "mu = 0.00005\n",
    "model = KernelPerceptron(mu)\n",
    "model.train(train_set, kernel)\n",
    "\n",
    "# Obtenir ses prÃ©dictions\n",
    "t1 = time.clock()\n",
    "les_sorties = model.compute_predictions(test_inputs, kernel)\n",
    "\n",
    "t2 = time.clock()\n",
    "print 'Ca nous a pris ', t2-t1, ' secondes pour calculer les predictions sur ', test_inputs.shape[0],' points de test'\n",
    "\n",
    "# Vote majoritaire (+1 puisquie nos classes sont de 1 a n)\n",
    "classes_pred = np.sign(les_sorties)\n",
    "\n",
    "# Faire les tests\n",
    "err = 1.0 - np.mean(test_labels==classes_pred)\n",
    "print \"L'erreur de test est de \", 100.0 * err,\"%\"\n",
    "\n",
    "if len(train_cols) == 2:\n",
    "    # Surface de decision\n",
    "    t1 = time.clock()\n",
    "    les_sorties = model_perceptron.compute_predictions(train_set[:,:-1],kernel)\n",
    "    train_classes_pred = np.sign(les_sorties)\n",
    "    plt.scatter(train_set[:,0],train_set[:,1],c=train_classes_pred)\n",
    "    plt.scatter(test_set[:,0],test_set[:,1],c=classes_pred)\n",
    "    plt.show()\n",
    "    \n",
    "    plt.scatter(train_set[:,0],train_set[:,1],c=train_set[:,-1])\n",
    "    plt.scatter(test_set[:,0],test_set[:,1],c=test_labels)\n",
    "    plt.show()\n",
    "    t2 = time.clock()\n",
    "    print 'Ca nous a pris ', t2-t1, ' secondes pour calculer les predictions sur ', grid_size * grid_size, ' points de la grille'\n",
    "    filename = 'grille_' + '_c1=' + str(train_cols[0]) + '_c2=' + str(train_cols[1])+'.png'\n",
    "    print 'On va sauvegarder la figure dans ', filename\n",
    "    pylab.savefig(filename,format='png')\n",
    "else:\n",
    "    print 'Trop de dimensions (', len(train_cols),') pour pouvoir afficher la surface de decision'\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
