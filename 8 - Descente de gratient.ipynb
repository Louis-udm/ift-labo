{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "#Descente de gradient\n",
      "\n",
      "Aujourd'hui nous allons consid\u00e9rer un mod\u00e8le lin\u00e9aire avec deux entr\u00e9es et une sortie binaire. Dans un premier temps, nous allons d\u00e9river a la main les \u00e9quations de l'algorithme de la descente de gradient pour trois fonctions de cout diff\u00e9rentes. Puis nous impl\u00e9menterons deux de ces algorithmes d'apprentissage dans le cadre de la classification de la base de donn\u00e9e iris.\n",
      "\n",
      "Le mod\u00e8le lin\u00e9aire s'\u00e9crit $f(x) = x_1 * w_1 + x_2 * w_2 + b$. Pour une \u00e9tiquette $y$ ($y=0$ ou $y=1$), le cout est not\u00e9 $E(f(x), y)$.\n",
      "\n",
      "Les trois fonctions de cout consider\u00e9es sont les suivantes:\n",
      "\n",
      "1) $E(f(x), y) = - I_{f(x) * y < 0} f(x) * y$\n",
      "\n",
      "2) l'erreur quadratique $E(f(x), y) = \\frac{1}{2} (f(x) - y)^2$\n",
      "\n",
      "3) la cross-entropy sigmoide $E(f(x), y) = - y \\ln (sigm(f(x))) - (1-y) \\ln (1-sigm(f(x))$ ou $sigm$ est la fonction sigmoide $sigm(x) = \\frac{1}{1+e^{-x}}$.\n",
      "\n",
      "##1. D\u00e9rivations\n",
      "\n",
      "D\u00e9rivez a la main les \u00e9quations de mise \u00e0 jour par la descente de gradient des param\u00e8tes $w_1$, $w_2$ et $b$ pour chacun des trois couts. Que reconnaissez-vous pour le cout 1)? \n",
      "\n",
      "##2. Impl\u00e9mentation\n",
      "\n",
      "###Pr\u00e9paration des donn\u00e9es\n",
      "\n",
      "Dans ce travail pratique on va travailler sur le dataset iris. On va utiliser seulement 2 classes: les iris avec les \u00e9tiquettes 1 et 2, que l'on va transformer en 1 et 0 pour les besoins du perceptron. On va \u00e9galement utiliser uniquement 2 traits par iris, afin de pouvoir visualiser l'algorithme.\n",
      "\n",
      "Voici le code qui pr\u00e9pare les donn\u00e9es. N'h\u00e9sitez pas \u00e0 regarder les *shapes* des diff\u00e9rents sets pour voir comment les donn\u00e9es sont pr\u00e9par\u00e9es!"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%pylab inline\n",
      "import numpy as np\n",
      "\n",
      "#On commence par charger iris\n",
      "iris = np.loadtxt('iris.txt')\n",
      "data = iris\n",
      "\n",
      "# On se limite au cas de la classification BINAIRE donc on va seulement garder \n",
      "# donn\u00e9es des 2 premi\u00e8res classes.\n",
      "# Ici on garde juste les exemples avec l'etiquette 1 et 2.\n",
      "data = data[data[:,-1]<3,:]\n",
      "# Ici on transforme chaque etiquette qui est egale a 2 en -1, pour avoir les \n",
      "# m\u00eames \u00e9tiquettes que dans la formulation standard du perceptron (1 et -1).\n",
      "data[data[:,-1]==2,-1] = 0\n",
      "\n",
      "# On se limite \u00e0 des donn\u00e9es dont la dimension est 2, de fa\u00e7on \u00e0 pouvoir visualiser\n",
      "# la fronti\u00e8re de decision avec la fonction gridplot.\n",
      "train_cols = [2,3]\n",
      "# Une variable pour contenir l'indice de la colonne correspondant aux \u00e9tiquettes.\n",
      "target_ind = [data.shape[1] - 1]\n",
      "\n",
      "# Nombre de classes\n",
      "n_classes = 2\n",
      "# Nombre de points d'entrainement\n",
      "n_train = 75\n",
      "\n",
      "# Commenter pour avoir des resultats non-deterministes \n",
      "np.random.seed(2)\n",
      "\n",
      "# D\u00e9terminer au hasard des indices pour les exemples d'entrainement et de test\n",
      "inds = range(data.shape[0])\n",
      "np.random.shuffle(inds)\n",
      "train_inds = inds[:n_train]\n",
      "test_inds = inds[n_train:]\n",
      "    \n",
      "# S\u00e9parer les donnees dans les deux ensembles: entrainement et test.\n",
      "train_set = data[train_inds,:]  # garder les bonnes lignes\n",
      "train_set = train_set[:,train_cols + target_ind]  # garder les bonnes colonnes\n",
      "test_set = data[test_inds,:]\n",
      "test_set = test_set[:,train_cols + target_ind]\n",
      "\n",
      "# Normaliser les donn\u00e9es\n",
      "mu1 = train_set[:,0].mean()\n",
      "mu2 = train_set[:,1].mean()\n",
      "sigma1 = train_set[:,0].std()\n",
      "sigma2 = train_set[:,1].std()\n",
      "train_set[:,0] -= mu1\n",
      "train_set[:,1] -= mu2\n",
      "train_set[:,0] /= sigma1\n",
      "train_set[:,1] /= sigma2\n",
      "test_set[:,0] -= mu1\n",
      "test_set[:,1] -= mu2\n",
      "test_set[:,0] /= sigma1\n",
      "test_set[:,1] /= sigma2\n",
      "\n",
      "# S\u00e9pararer l'ensemble de test: entr\u00e9es et \u00e9tiquettes.\n",
      "test_inputs = test_set[:,:-1]\n",
      "test_labels = test_set[:,-1]\n",
      "\n",
      "# Le taux d'apprentissage\n",
      "mu = 0.001\n",
      "\n",
      "# Le nombre max d'it\u00e9rations\n",
      "max_iter = 500\n",
      "\n",
      "# La classe parente de nos mod\u00e8les\n",
      "class Model:\n",
      "\n",
      "    def plot_function(self, train_data, title):\n",
      "        plt.figure()\n",
      "        d1 = train_data[train_data[:, -1] > 0]\n",
      "        d2 = train_data[train_data[:, -1] == 0]\n",
      "        plt.scatter(d1[:, 0], d1[:, 1], c='b', label='classe 1')\n",
      "        plt.scatter(d2[:, 0], d2[:, 1], c='g', label='classe 0')\n",
      "        x = np.linspace(-10, 10, 100)\n",
      "        y = -(self.weights[0]*x + self.bias)/self.weights[1]\n",
      "        plt.plot(x, y, c='r', lw=2, label='y = -(w1*x + b1)/w2')\n",
      "        plt.xlim(np.min(train_data[:, 0]) - 0.5, np.max(train_data[:, 0]) + 0.5)\n",
      "        plt.ylim(np.min(train_data[:, 1]) - 0.5, np.max(train_data[:, 1]) + 0.5)\n",
      "        plt.grid()\n",
      "        plt.legend(loc='lower right')\n",
      "        plt.title(title)\n",
      "        plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###LinearRegression et SigmoidCrossEntropy###\n",
      "\n",
      "On introduit les classes *LinearRegression* (erreur quadratique) et *SigmoidCrossEntropy* (cross-entropy sigmoid). Comme d'habitude, ce sont des algorithmes qui poss\u00e8dent une fonction *train* pour entra\u00eener l'algorithme \u00e0 partir du *train_set* et une fonction *compute_prediction*, qui pr\u00e9dit la classe de chaque exemple de *test_inputs*.\n",
      "\n",
      "On vous demande de compl\u00e9ter les 2 algorithmes. Il faut compl\u00e9ter leur fonctions *train* avec les gradients que vous avez calcul\u00e9 pr\u00e9c\u00e9demment, ainsi que leur fonctions *compute_prediction*. Nous vous laissons aussi le soin d'impl\u00e9menter la boucle d'entrainement (Attention \u00e0 bien utiliser *max_iter*, pour \u00e9viter que votre algorithme ne s'arr\u00eate jamais!)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class LinearRegression(Model):\n",
      "    def __init__(self, mu, max_iter):\n",
      "        self.mu = mu\n",
      "        self.max_iter = max_iter\n",
      "    \n",
      "    def train(self, train_data):\n",
      "        nb_example = train_data.shape[0]\n",
      "        self.weights = np.random.randn(train_data.shape[1]-1)\n",
      "        self.bias = np.zeros(1)\n",
      "        pass\n",
      "\n",
      "    def compute_predictions(self, test_data):\n",
      "        pass\n",
      "\n",
      "\n",
      "class SigmoidCrossEntropy(Model):\n",
      "    def __init__(self, mu, max_iter):\n",
      "        self.mu = mu\n",
      "        self.max_iter = max_iter\n",
      "    \n",
      "    def train(self, train_data):\n",
      "        nb_example = train_data.shape[0]\n",
      "        self.weights = np.random.random(train_data.shape[1])\n",
      "        self.bias = np.zeros(1)\n",
      "        pass\n",
      "\n",
      "    def compute_predictions(self, test_data):\n",
      "        pass"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###Entra\u00eenement et test du mod\u00e8le###\n",
      "\n",
      "Lorsqu'une des classes ci-dessus est compl\u00e9t\u00e9e, vous peut l'entra\u00eener et le tester!"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Cr\u00e9er et entrainer le modele\n",
      "model = LinearRegression(mu, max_iter)\n",
      "model.train(train_set)\n",
      "model.plot_function(train_set, 'Train data')\n",
      "\n",
      "\n",
      "# Obtenir les classes pr\u00e9dites sur l'ensemble de test\n",
      "predictions = model.compute_predictions(test_inputs)\n",
      "\n",
      "# Convertir les sorties en classe. On prend le signe.\n",
      "classes_pred = np.sign(predictions-0.5)\n",
      "   \n",
      "# Mesurer la performance.\n",
      "err = 1.0 - np.mean(test_labels==classes_pred)\n",
      "\n",
      "model.plot_function(test_set, 'Test data')\n",
      "print \"L'erreur de test est de \", 100.0 * err,\"%\" "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}